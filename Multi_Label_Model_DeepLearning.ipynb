{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import common librarie\n",
    "from zipfile import ZipFile \n",
    "import os.path\n",
    "from os import path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import base64\n",
    "import re\n",
    "from itertools import groupby\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>dynasty</th>\n",
       "      <th>author</th>\n",
       "      <th>tags</th>\n",
       "      <th>star</th>\n",
       "      <th>author_stars</th>\n",
       "      <th>title</th>\n",
       "      <th>tags_list</th>\n",
       "      <th>new_tags</th>\n",
       "      <th>new_first_tag</th>\n",
       "      <th>写物</th>\n",
       "      <th>劝勉</th>\n",
       "      <th>家庭</th>\n",
       "      <th>快乐</th>\n",
       "      <th>悲苦</th>\n",
       "      <th>政治</th>\n",
       "      <th>朋友</th>\n",
       "      <th>游玩</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>《吴都赋》云：“户藏烟浦，家具画船。”唯吴兴为然。春游之盛，西湖未能过也。己酉岁，予与萧时父...</td>\n",
       "      <td>宋代</td>\n",
       "      <td>姜夔</td>\n",
       "      <td>春游;怀人;</td>\n",
       "      <td>64</td>\n",
       "      <td>279</td>\n",
       "      <td>琵琶仙·《吴都赋》云：「户藏烟浦</td>\n",
       "      <td>['春游', '怀人']</td>\n",
       "      <td>政治;游玩</td>\n",
       "      <td>政治</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>《廿一史弹词》第三段说秦汉开场词滚滚长江东逝水，浪花淘尽英雄。是非成败转头空。青山依旧在，几...</td>\n",
       "      <td>明代</td>\n",
       "      <td>杨慎</td>\n",
       "      <td>咏史;抒怀;人生;哲理</td>\n",
       "      <td>3244</td>\n",
       "      <td>131</td>\n",
       "      <td>临江仙·滚滚长江东逝水</td>\n",
       "      <td>['咏史', '抒怀', '人生', '哲理']</td>\n",
       "      <td>写物;劝勉;悲苦;政治</td>\n",
       "      <td>写物</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>《水经》云：“彭蠡之口有石钟山焉。”郦元以为下临深潭，微风鼓浪，水石相搏，声如洪钟。是说也，...</td>\n",
       "      <td>宋代</td>\n",
       "      <td>苏轼</td>\n",
       "      <td>古文观止;纪游;写景;写山</td>\n",
       "      <td>306</td>\n",
       "      <td>4011</td>\n",
       "      <td>石钟山记</td>\n",
       "      <td>['古文观止', '纪游', '写景', '写山']</td>\n",
       "      <td>写物;政治;游玩</td>\n",
       "      <td>写物</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>【序】辛亥之冬，予载雪诣石湖。止既月，授简索句，且征新声，作此两曲。石湖把玩不已，使工妓隶习...</td>\n",
       "      <td>宋代</td>\n",
       "      <td>姜夔</td>\n",
       "      <td>咏物;梅花</td>\n",
       "      <td>74</td>\n",
       "      <td>279</td>\n",
       "      <td>暗香疏影</td>\n",
       "      <td>['咏物', '梅花']</td>\n",
       "      <td>劝勉</td>\n",
       "      <td>劝勉</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>〔一枝花〕　　攀出墙朵朵花，折临路枝枝柳。花攀红蕊嫩，柳折翠条柔，浪子风流。凭着我折柳攀花手...</td>\n",
       "      <td>元代</td>\n",
       "      <td>关汉卿</td>\n",
       "      <td>;散曲;抒情;生活</td>\n",
       "      <td>140</td>\n",
       "      <td>124</td>\n",
       "      <td>【南吕】一枝花不伏老</td>\n",
       "      <td>['散曲', '抒情', '生活']</td>\n",
       "      <td>写物;快乐;悲苦</td>\n",
       "      <td>写物</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content dynasty author  \\\n",
       "0  《吴都赋》云：“户藏烟浦，家具画船。”唯吴兴为然。春游之盛，西湖未能过也。己酉岁，予与萧时父...      宋代     姜夔   \n",
       "1  《廿一史弹词》第三段说秦汉开场词滚滚长江东逝水，浪花淘尽英雄。是非成败转头空。青山依旧在，几...      明代     杨慎   \n",
       "2  《水经》云：“彭蠡之口有石钟山焉。”郦元以为下临深潭，微风鼓浪，水石相搏，声如洪钟。是说也，...      宋代     苏轼   \n",
       "3  【序】辛亥之冬，予载雪诣石湖。止既月，授简索句，且征新声，作此两曲。石湖把玩不已，使工妓隶习...      宋代     姜夔   \n",
       "4  〔一枝花〕　　攀出墙朵朵花，折临路枝枝柳。花攀红蕊嫩，柳折翠条柔，浪子风流。凭着我折柳攀花手...      元代    关汉卿   \n",
       "\n",
       "            tags  star  author_stars             title  \\\n",
       "0         春游;怀人;    64           279  琵琶仙·《吴都赋》云：「户藏烟浦   \n",
       "1    咏史;抒怀;人生;哲理  3244           131       临江仙·滚滚长江东逝水   \n",
       "2  古文观止;纪游;写景;写山   306          4011              石钟山记   \n",
       "3          咏物;梅花    74           279              暗香疏影   \n",
       "4      ;散曲;抒情;生活   140           124        【南吕】一枝花不伏老   \n",
       "\n",
       "                    tags_list     new_tags new_first_tag  写物  劝勉  家庭  快乐  悲苦  \\\n",
       "0                ['春游', '怀人']        政治;游玩            政治   0   0   0   0   0   \n",
       "1    ['咏史', '抒怀', '人生', '哲理']  写物;劝勉;悲苦;政治            写物   1   1   0   0   1   \n",
       "2  ['古文观止', '纪游', '写景', '写山']     写物;政治;游玩            写物   1   0   0   0   0   \n",
       "3                ['咏物', '梅花']           劝勉            劝勉   0   1   0   0   0   \n",
       "4          ['散曲', '抒情', '生活']     写物;快乐;悲苦            写物   1   0   0   1   1   \n",
       "\n",
       "   政治  朋友  游玩  \n",
       "0   1   0   1  \n",
       "1   1   0   0  \n",
       "2   1   0   1  \n",
       "3   0   0   0  \n",
       "4   0   0   0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = \"poems_with_new_tags.zip\"\n",
    "poem_df = pd.read_csv('poems_with_new_tags.zip', compression='zip', header=0, quotechar='\"')\n",
    "poem_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-3-b2557c350962>, line 37)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-b2557c350962>\"\u001b[0;36m, line \u001b[0;32m37\u001b[0m\n\u001b[0;31m    for poem in poem_contents:\u001b[0m\n\u001b[0m                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "# Pre-processed content for bert embdedding\n",
    "def processed_content_with_multi_sentences(poem_df):\n",
    "    poem_contents = poem_df['content'].to_list()\n",
    "    processed_poem_contents = []\n",
    "    local_max_length = 0\n",
    "    max_length = 30\n",
    "    max_length_sentence = \"\"\n",
    "    for poem in poem_contents:\n",
    "        split_sentences = re.split('。|！|？',poem)\n",
    "        prcessed_sentences = []\n",
    "        for sentence in split_sentences:\n",
    "            sentence = sentence.strip()\n",
    "            sentence = sentence.replace(\"\\u3000\",\" \")\n",
    "            if \"“\" in sentence and \"”\" not in sentence:\n",
    "                sentence = sentence + \"”\"\n",
    "            elif \"”\" in sentence and \"“\" not in sentence:\n",
    "                sentence = sentence[1:]\n",
    "            elif len(re.findall(r'[\\u4e00-\\u9fff]+', sentence)) < 1:\n",
    "                continue\n",
    "            if len(sentence) > local_max_length:\n",
    "                max_length_sentence = sentence\n",
    "                local_max_length = max(local_max_length, len(sentence))\n",
    "            sentence = \"[CLS]\" + sentence + \"。\"\n",
    "            prcessed_sentences.append(sentence)\n",
    "\n",
    "        processed_poem_string = \"[SEP]\".join(prcessed_sentences)\n",
    "        processed_poem_contents.append(processed_poem_string)\n",
    "\n",
    "    print(local_max_length)  \n",
    "    print(max_length_sentence)\n",
    "\n",
    "def processed_content_with_one_poems(poem_df):\n",
    "    poem_contents = poem_df['content'].to_list()\n",
    "    processed_poem_contents = []\n",
    "    max_length = 512\n",
    "    max_length_sentence = \"\"\n",
    "    for poem in poem_contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "poem_df[\"processed_content\"] = processed_poem_contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "poems_info = poem_df[['content','dynasty','author','title']]\n",
    "tags = poem_df.drop(['content','dynasty','author','title','star','author_stars'], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(poems_info, tags, test_size=0.33)\n",
    "y_train_final = y_train.drop(['tags','tags_list','new_tags','new_first_tag'], axis=1)\n",
    "y_test_final = y_test.drop(['tags','tags_list','new_tags','new_first_tag'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train)==len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>dynasty</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3389</th>\n",
       "      <td>汉昭帝逝世，昌邑王刘贺被废黜，汉宣帝刘询刚刚登上皇位。路温舒呈上奏书，奏书说：　　昭帝崩，昌...</td>\n",
       "      <td>两汉</td>\n",
       "      <td>路温舒</td>\n",
       "      <td>尚德缓刑书</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>买陂塘、旋栽杨柳，依稀淮岸江浦。东皋嘉雨新痕涨，沙觜鹭来鸥聚。堪爱处最好是、一川夜月光流渚。...</td>\n",
       "      <td>宋代</td>\n",
       "      <td>晁补之</td>\n",
       "      <td>摸鱼儿·东皋寓居</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4665</th>\n",
       "      <td>胜神鳌，夯风涛，脊梁上轻负着蓬莱岛。万里夕阳锦背高，翻身犹恨东洋小，太公怎钓？</td>\n",
       "      <td>元代</td>\n",
       "      <td>王和卿</td>\n",
       "      <td>拨不断·大鱼</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4632</th>\n",
       "      <td>老农家贫在山住，耕种山田三四亩。苗疏税多不得食，输入官仓化为土。岁暮锄犁傍空室，呼儿登山收橡...</td>\n",
       "      <td>唐代</td>\n",
       "      <td>张籍</td>\n",
       "      <td>野老歌山农词</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4250</th>\n",
       "      <td>离人无语月无声，明月有光人有情。别后相思人似月，云间水上到层城。</td>\n",
       "      <td>唐代</td>\n",
       "      <td>李冶</td>\n",
       "      <td>明月夜留别</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                content dynasty author  \\\n",
       "3389  汉昭帝逝世，昌邑王刘贺被废黜，汉宣帝刘询刚刚登上皇位。路温舒呈上奏书，奏书说：　　昭帝崩，昌...      两汉    路温舒   \n",
       "396   买陂塘、旋栽杨柳，依稀淮岸江浦。东皋嘉雨新痕涨，沙觜鹭来鸥聚。堪爱处最好是、一川夜月光流渚。...      宋代    晁补之   \n",
       "4665            胜神鳌，夯风涛，脊梁上轻负着蓬莱岛。万里夕阳锦背高，翻身犹恨东洋小，太公怎钓？      元代    王和卿   \n",
       "4632  老农家贫在山住，耕种山田三四亩。苗疏税多不得食，输入官仓化为土。岁暮锄犁傍空室，呼儿登山收橡...      唐代     张籍   \n",
       "4250                   离人无语月无声，明月有光人有情。别后相思人似月，云间水上到层城。      唐代     李冶   \n",
       "\n",
       "         title  \n",
       "3389     尚德缓刑书  \n",
       "396   摸鱼儿·东皋寓居  \n",
       "4665    拨不断·大鱼  \n",
       "4632    野老歌山农词  \n",
       "4250     明月夜留别  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4091, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>写物</th>\n",
       "      <th>劝勉</th>\n",
       "      <th>家庭</th>\n",
       "      <th>快乐</th>\n",
       "      <th>悲苦</th>\n",
       "      <th>政治</th>\n",
       "      <th>朋友</th>\n",
       "      <th>游玩</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3389</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4665</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4632</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4250</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      写物  劝勉  家庭  快乐  悲苦  政治  朋友  游玩\n",
       "3389   1   0   0   0   0   1   0   0\n",
       "396    0   1   0   0   0   1   0   1\n",
       "4665   0   1   0   0   1   1   0   0\n",
       "4632   1   0   0   1   1   1   0   1\n",
       "4250   1   0   0   0   0   0   1   0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4091, 8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, RobertaTokenizer, DistilBertTokenizer,TFBertModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_poem_to_feature(review):\n",
    "    bert_input = bert_tokenizer.encode_plus(review, \n",
    "                                 add_special_tokens = True, # add [CLS], [SEP]\n",
    "                                 max_length = 512,\n",
    "                                 pad_to_max_length = True, # add [PAD] tokens\n",
    "                                 return_attention_mask = True # add attention mask to not focus on pad tokens\n",
    "                                )\n",
    "    return bert_input\n",
    "\n",
    "# map to the expected input to TFBertForSequenceClassification, see here \n",
    "def map_poem_to_dict(input_ids, attention_masks, token_type_ids, label):\n",
    "    return {\n",
    "      \"input_ids\": input_ids,\n",
    "      \"token_type_ids\": token_type_ids,\n",
    "      \"attention_mask\": attention_masks,\n",
    "  }, label\n",
    "\n",
    "def encode_poems(poems, labels,limit=-1):\n",
    "    embedding_weight = []\n",
    "    input_ids_list = []\n",
    "    token_type_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    label_list = []\n",
    "    # prepare list, so that we can build up final TensorFlow dataset from slices.\n",
    "    if (limit > 0):\n",
    "        ds = ds.take(limit)\n",
    "\n",
    "    for poem, label in zip(poems['content'],labels):\n",
    "        bert_input = convert_poem_to_feature(poem)\n",
    "        input_ids_list.append(bert_input['input_ids'])\n",
    "        token_type_ids_list.append(bert_input['token_type_ids'])\n",
    "        attention_mask_list.append(bert_input['attention_mask'])\n",
    "        label_list.append(label.tolist())\n",
    "    return (input_ids_list, token_type_ids_list, label_list)    \n",
    "#     return tf.data.Dataset.from_tensor_slices((input_ids_list, \n",
    "#                                                attention_mask_list, \n",
    "#                                                token_type_ids_list, label_list)).map(map_poem_to_dict)\n",
    "                                                                                                                         \n",
    "                                                                                                                         \n",
    "                                                                                                                         \n",
    "                                                                                                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 0, 0, 0, 0, 1, 0, 0], [0, 1, 0, 0, 0, 1, 0, 1], [0, 1, 0, 0, 1, 1, 0, 0], [1, 0, 0, 1, 1, 1, 0, 1], [1, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 1, 1, 0, 1, 0], [1, 0, 0, 0, 1, 1, 0, 0], [0, 0, 0, 0, 1, 0, 0, 1], [1, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 1, 1, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "small_example_train = X_train[:10]\n",
    "small_example_label = y_train_final[:10].to_numpy()\n",
    "result= encode_poems(small_example_train,small_example_label)\n",
    "print(result[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train dataset\n",
    "batch_size = 64\n",
    "ds_train_encoded, ds_train_labels = encode_poems(X_train,y_train_final.to_numpy())\n",
    "# test dataset\n",
    "ds_test_encoded, ds_test_labels = encode_poems(X_test, y_test_final.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21128"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bert_tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.metrics import accuracy_score\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten, LSTM\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Input\n",
    "from keras.layers.merge import Concatenate\n",
    "from transformers import TFBertForSequenceClassification\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # recommended learning rate for Adam 5e-5, 3e-5, 2e-5\n",
    "# learning_rate = 2e-5\n",
    "# # we will do just 1 epoch for illustration, though multiple epochs might be better as long as we will not overfit the model\n",
    "# number_of_epochs = 8\n",
    "\n",
    "# # model initialization\n",
    "# model = TFBertForSequenceClassification.from_pretrained('bert-base-chinese', num_labels=8)\n",
    "\n",
    "# # optimizer Adam recommended\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate,epsilon=1e-08, clipnorm=1)\n",
    "\n",
    "# # we do not have one-hot vectors, we can use sparce categorical cross entropy and accuracy\n",
    "# loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "# metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "# model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "# # fit model\n",
    "# bert_history = model.fit(ds_train_encoded, epochs=number_of_epochs)\n",
    "# # evaluate test set\n",
    "# model.evaluate(ds_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = Tokenizer(num_words=5000)\n",
    "# tokenizer.fit_on_texts(X_train_tokenized)\n",
    "\n",
    "# X_train = tokenizer.texts_to_sequences(X_train_tokenized)\n",
    "# X_test = tokenizer.texts_to_sequences(X_test_tokenized)\n",
    "\n",
    "# vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# maxlen = 200\n",
    "\n",
    "# X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "# X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "# embeddings_dictionary = dict()\n",
    "\n",
    "# glove_file = open('/content/drive/My Drive/Colab Datasets/glove.6B.100d.txt', encoding=\"utf8\")\n",
    "\n",
    "# for line in glove_file:\n",
    "#     records = line.split()\n",
    "#     word = records[0]\n",
    "#     vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "#     embeddings_dictionary[word] = vector_dimensions\n",
    "# glove_file.close()\n",
    "\n",
    "# embedding_matrix = zeros((vocab_size, 100))\n",
    "# for word, index in tokenizer.word_index.items():\n",
    "#     embedding_vector = embeddings_dictionary.get(word)\n",
    "#     if embedding_vector is not None:\n",
    "#         embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-chinese were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-chinese.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# bert_layer = TFBertModel.from_pretrained('bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maxlen = 200\n",
    "# tokenizer = Tokenizer(num_words=5000)\n",
    "# tokenizer.fit_on_texts(X_train_tokenized)\n",
    "# X_train = tokenizer.texts_to_sequences(X_train_tokenized)\n",
    "# X_test = tokenizer.texts_to_sequences(X_test_tokenized)\n",
    "# X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "# X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This model has not yet been built. Build the model first by calling `build()` or calling `fit()` with some data, or specify an `input_shape` argument in the first layer(s) for automatic build.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-ef6301842efc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# summarize the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;31m# fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_train_encoded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds_train_encoded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(self, line_length, positions, print_fn)\u001b[0m\n\u001b[1;32m   2349\u001b[0m     \"\"\"\n\u001b[1;32m   2350\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2351\u001b[0;31m       raise ValueError('This model has not yet been built. '\n\u001b[0m\u001b[1;32m   2352\u001b[0m                        \u001b[0;34m'Build the model first by calling `build()` or calling '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2353\u001b[0m                        \u001b[0;34m'`fit()` with some data, or specify '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: This model has not yet been built. Build the model first by calling `build()` or calling `fit()` with some data, or specify an `input_shape` argument in the first layer(s) for automatic build."
     ]
    }
   ],
   "source": [
    "deep_inputs = Input(shape=(513,))\n",
    "embedding_layer = Embedding(21128, 100, weights=[embedding_matrix], trainable=False)(deep_inputs)\n",
    "LSTM_Layer_1 = LSTM(128)(embedding_layer)\n",
    "dense_layer_1 = Dense(6, activation='sigmoid')(LSTM_Layer_1)\n",
    "model = Model(inputs=deep_inputs, outputs=dense_layer_1)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(Embedding(21128, 64))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "# # compile the model\n",
    "# model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "# # summarize the model\n",
    "# print(model.summary())\n",
    "# # fit the model\n",
    "# model.fit(ds_train_encoded, epochs=50, verbose=0)\n",
    "# # evaluate the model\n",
    "# loss, accuracy = model.evaluate(ds_test_encoded, verbose=0)\n",
    "# print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep_inputs = Input(shape=(maxlen,))\n",
    "# embedding_layer = Embedding(34345, 100, weights=[embedding_matrix], trainable=False)(deep_inputs)\n",
    "# LSTM_Layer_1 = LSTM(128)(embedding_layer)\n",
    "# dense_layer_1 = Dense(6, activation='sigmoid')(LSTM_Layer_1)\n",
    "# model = Model(inputs=deep_inputs, outputs=dense_layer_1)\n",
    "\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
